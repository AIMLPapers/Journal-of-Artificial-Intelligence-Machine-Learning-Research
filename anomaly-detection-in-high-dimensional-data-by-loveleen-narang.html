<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Anomaly Detection in High-Dimensional Data</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <script>
        MathJax = {
          tex: {
            inlineMath: [['\\(', '\\)']],
            displayMath: [['$$', '$$']],
            processEscapes: true,
            processEnvironments: true,
            tags: 'ams' // For equation numbering
          },
          options: {
            skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
          }
        };
      </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
   <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
            color: #333;
        }
        .container {
            max-width: 1000px;
            margin: 20px auto;
            padding: 20px;
            background-color: #ffffff;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
            border-radius: 8px;
        }
        .intro-section {
            background-color: white;
            padding: 30px;
            margin: -20px -20px 20px -20px; /* Extend to container edges */
            border-radius: 8px 8px 0 0;
            text-align: center;
            border-bottom: 1px solid #eee;
        }
        .intro-section h1 {
            font-size: 2.5em;
            color: #2c3e50;
            margin-bottom: 10px;
        }
        .intro-section .catch-phrase {
            font-size: 1.2em;
            color: #3498db;
            margin-bottom: 15px;
            font-style: italic;
        }
        .intro-section i.fas {
            font-size: 3em;
            color: #3498db;
            margin-bottom: 15px;
        }
        h2 {
            color: #3498db;
            border-bottom: 2px solid #3498db;
            padding-bottom: 5px;
            margin-top: 30px;
        }
        h3 {
            color: #2980b9;
            margin-top: 25px;
        }
        p, li {
            color: #555;
        }
        strong {
            color: #2c3e50;
        }
        code {
            background-color: #eee;
            padding: 2px 5px;
            border-radius: 3px;
            font-family: 'Courier New', Courier, monospace;
        }
        .formula {
            display: block;
            background-color: #eaf2f8;
            padding: 15px;
            margin: 15px 0;
            border-left: 5px solid #3498db;
            overflow-x: auto;
            font-size: 1.1em;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 25px 0;
            box-shadow: 0 2px 3px rgba(0,0,0,0.1);
        }
        th, td {
            padding: 12px 15px;
            border: 1px solid #ddd;
            text-align: left;
        }
        th {
            background-color: #3498db;
            color: white;
            font-weight: bold;
        }
        tr:nth-child(even) {
            background-color: #f2f2f2;
        }
        tr:hover {
            background-color: #eaf2f8;
        }
        .svg-diagram {
            display: block;
            margin: 20px auto;
            text-align: center;
        }
        .author-box {
            background-color: #eaf2f8;
            padding: 20px;
            margin-top: 40px;
            border-radius: 5px;
            border-left: 5px solid #2980b9;
        }
        .author-box h2 {
            border-bottom: none;
            margin-top: 0;
        }
        .author-box p {
            color: #333;
        }
    </style>
</head>
<body>

<div class="container">

    <header class="intro-section">
        <i class="fas fa-exclamation-triangle"></i> <h1>Anomaly Detection in High-Dimensional Data</h1>
        <p class="sub-title">Finding Needles in High-Dimensional Haystacks: Techniques and Hurdles</p>
        <p><strong>Authored by:</strong> Loveleen Narang</p>
        <p><strong>Date:</strong> November 2, 2024</p>
    </header>

    <h2><i class="fas fa-search-plus icon"></i> Introduction: The Quest for the Unusual</h2>
    <p>
        <strong>Anomaly Detection</strong>, also known as outlier detection, is the task of identifying data points, events, or observations that deviate significantly from the majority of the data, raising suspicions by differing from normal behavior. These anomalies can represent critical incidents such as fraudulent transactions, network intrusions, manufacturing defects, system failures, or novel medical conditions.
    </p>
    <p>
        While anomaly detection is challenging in itself, it becomes significantly harder when dealing with <strong>High-Dimensional Data</strong> – datasets where each data point \( x \) is described by a large number of features or dimensions \( d \) (Formula 1: \( x \in \mathbb{R}^d, d \gg 1 \)). Modern datasets from domains like cybersecurity, finance, bioinformatics, and sensor networks often possess hundreds or thousands of dimensions. This high dimensionality introduces unique challenges collectively known as the <strong>"Curse of Dimensionality"</strong>, rendering many traditional anomaly detection techniques ineffective. This article explores the specific challenges and prominent techniques for tackling anomaly detection in high-dimensional spaces.
    </p>

    <h2><i class="fas fa-cubes icon challenge-icon"></i> The Curse of Dimensionality: Why High Dimensions are Hard</h2>
    <p>As the number of dimensions \( d \) increases, several phenomena emerge that complicate anomaly detection:</p>
    <ul>
        <li><i class="fas fa-ruler-combined challenge-icon"></i><strong>Distance Concentration:</strong> In high dimensions, the distances between pairs of points tend to become statistically indistinguishable. Standard distance metrics like Euclidean distance (Formula 2: \( ||x - y||_2 = \sqrt{\sum (x_i - y_i)^2} \)) lose their discriminative power, making it hard to separate outliers based solely on distance. Manhattan distance (Formula 3: \( ||x - y||_1 = \sum |x_i - y_i| \)) can sometimes be less affected.</li>
        <li><i class="fas fa-border-none challenge-icon"></i><strong>Data Sparsity:</strong> The volume of the space grows exponentially with dimensionality. A fixed number of data points becomes increasingly sparse, making density estimation extremely difficult and unreliable.</li>
        <li><i class="fas fa-unlink challenge-icon"></i><strong>Irrelevant Features:</strong> High-dimensional data often contains many irrelevant or noisy features that can mask the anomalous nature of a data point in the relevant dimensions. Anomalies might only be apparent in specific low-dimensional subspaces.</li>
        <li><i class="fas fa-project-diagram challenge-icon"></i><strong>Subspace Anomalies:</strong> Anomalies might not be outliers in the full feature space but only within specific combinations (subspaces) of features.</li>
        <li><i class="fas fa-calculator challenge-icon"></i><strong>Computational Cost:</strong> Many algorithms scale poorly with the number of dimensions, increasing computational time and memory requirements significantly.</li>
    </ul>
     <div class="svg-diagram">
        <h3>Curse of Dimensionality Illustration (Distance Concentration)</h3>
        <svg width="500" height="200" xmlns="http://www.w3.org/2000/svg">
            <defs><style>.label{font-size:12px;}.point{fill:#3498db;}.dist-line{stroke:#e74c3c; stroke-width:1.5; stroke-dasharray:4,4;}</style></defs>
            <g id="lowD">
                <text x="100" y="30" class="label" font-weight="bold">Low Dimensions (e.g., 2D)</text>
                <rect x="20" y="50" width="160" height="120" fill="#f8f9fa" stroke="#dee2e6"/>
                <circle cx="60" cy="140" r="4" class="point"/> <text x="40" y="145" class="label">A</text>
                <circle cx="140" cy="140" r="4" class="point"/> <text x="160" y="145" class="label">B</text>
                <circle cx="100" cy="80" r="4" class="point"/> <text x="100" y="70" class="label">C</text>
                <line x1="60" y1="140" x2="140" y2="140" class="dist-line"/> <text x="100" y="155" class="label" fill="#e74c3c">Dist(A,B)</text>
                <line x1="60" y1="140" x2="100" y2="80" class="dist-line"/> <text x="60" y="100" class="label" fill="#e74c3c">Dist(A,C)</text>
                <text x="100" y="185" class="label">Distances vary significantly</text>
            </g>
             <g id="highD" transform="translate(280,0)">
                 <text x="110" y="30" class="label" font-weight="bold">High Dimensions</text>
                  <rect x="20" y="50" width="180" height="120" fill="#f8f9fa" stroke="#dee2e6"/>
                 <circle cx="110" cy="110" r="4" class="point"/> <text x="90" y="115" class="label">A'</text>
                 <circle cx="120" cy="100" r="4" class="point"/> <text x="140" y="105" class="label">B'</text>
                 <circle cx="100" cy="120" r="4" class="point"/> <text x="80" y="125" class="label">C'</text>
                 <text x="110" y="160" class="label">Dist(A', B') ≈ Dist(A', C') ≈ Dist(B', C')</text>
                 <text x="110" y="185" class="label">Distances become similar ("Concentrated")</text>
            </g>
        </svg>
         <p class="caption">Fig 1: Conceptual illustration of distance concentration in high dimensions.</p>
     </div>

    <h2><i class="fas fa-filter icon"></i> Techniques for High-Dimensional Anomaly Detection</h2>
    <p>Various strategies have been developed to specifically address the challenges of high dimensionality.</p>

    <h3>Projection-Based Methods</h3>
    <p>These methods project the high-dimensional data onto a lower-dimensional space where anomalies might be easier to detect, assuming anomalies behave differently under projection.</p>
    <ul>
        <li><strong>Principal Component Analysis (PCA):</strong> Finds principal components (directions of maximum variance). Normal data is expected to project well onto the principal subspace spanned by the first \( k \) components, while anomalies might deviate significantly.
            <ul>
                <li>Process: Compute covariance matrix \( \Sigma \), find eigenvectors (principal components \( W \)), project data \( z = W_k^T (x - \mu) \). Formulas (7, 8, 9): Mean \( \mu \), Covariance \( \Sigma \), Eigenvectors \( W \).</li>
                <li>Anomaly Score: Can be based on the reconstruction error (distance between original point \( x \) and its projection back from the subspace \( \hat{x} = W_k z + \mu \)) or the distance in the residual subspace. Formula (10): Reconstruction Error \( ||x - \hat{x}||^2 \).</li>
            </ul>
        </li>
        <li><strong>Random Projections:</strong> Projects data onto random lower-dimensional subspaces. Faster than PCA, relies on the Johnson-Lindenstrauss lemma which states that distances are approximately preserved under random projection. Anomalies might stand out consistently across multiple random projections.</li>
    </ul>
     <div class="svg-diagram">
        <h3>PCA-based Anomaly Detection Concept</h3>
        <svg width="500" height="200" xmlns="http://www.w3.org/2000/svg">
             <defs><style>.point{fill:#3498db;}.anomaly{fill:#e74c3c;}.line{stroke:#555; stroke-width:1.5;}.proj-line{stroke:#aaa; stroke-width:1; stroke-dasharray:2,2;}.label{font-size:10px;}</style><marker id="arrowhead-pca" markerWidth="6" markerHeight="4" refX="5" refY="2" orient="auto"><polygon points="0 0, 6 2, 0 4" fill="#555" /></marker></defs>
            <ellipse cx="100" cy="100" rx="60" ry="30" fill="#eaf2f8" transform="rotate(-30 100 100)"/>
             <text x="100" y="30" class="label">Original High-D Data (Shown in 2D)</text>
             <circle cx="100" cy="100" r="3" class="point"/> <circle cx="120" cy="90" r="3" class="point"/> <circle cx="80" cy="110" r="3" class="point"/> <circle cx="130" cy="80" r="3" class="point"/> <circle cx="70" cy="120" r="3" class="point"/>
             <circle cx="180" cy="150" r="4" class="anomaly"/> <text x="190" y="155" class="label anomaly">Anomaly</text>
             <line x1="20" y1="141.2" x2="180" y2="58.8" class="line" marker-end="url(#arrowhead-pca)"/>
             <text x="160" y="45" class="label">Principal Component (PC1)</text>
            <line x1="100" y1="100" x2="88.8" y2="122.4" class="proj-line"/> <circle cx="88.8" cy="122.4" r="2" fill="#5dade2"/>
            <line x1="130" y1="80" x2="125.6" y2="88.8" class="proj-line"/> <circle cx="125.6" cy="88.8" r="2" fill="#5dade2"/>
            <line x1="180" y1="150" x2="127.6" y2="154.8" class="proj-line"/> <circle cx="127.6" cy="154.8" r="2" fill="#c0392b"/>
             <text x="100" y="180" class="label">Normal points project close to subspace</text>
             <text x="230" y="170" class="label anomaly">Anomaly has large projection error / distance</text>
             <line x1="180" y1="150" x2="127.6" y2="154.8" stroke="#e74c3c" stroke-width="1"/>
             <text x="153" y="160" class="label anomaly">Error</text>
             <line x1="65" y1="70" x2="135" y2="130" stroke="#bdc3c7" stroke-width="1" stroke-dasharray="3,3"/>
             <text x="145" y="140" class="label" fill="#7f8c8d">(Residual Subspace)</text>
        </svg>
        <p class="caption">Fig 2: PCA projects data onto principal components; anomalies often have large reconstruction errors.</p>
    </div>

    <h3>Subspace and Feature Selection Methods</h3>
    <p>These methods assume anomalies are only visible in certain subsets of features (subspaces) and try to identify these relevant subspaces.</p>
    <ul>
        <li><strong>Subspace Search:</strong> Algorithms like HiCS (High Contrast Subspaces) search for subspaces where the data distribution significantly differs from the full space, potentially highlighting anomalies.</li>
        <li><strong>Feature Bagging:</strong> Trains multiple anomaly detectors on random subsets of features and combines their outlier scores (e.g., by averaging). This makes the detection more robust to irrelevant features.</li>
    </ul>

    <h3>Distance and Density-Based Methods (with High-D Caveats)</h3>
    <p>While standard distance/density methods suffer in high dimensions, adaptations exist.</p>
    <ul>
        <li><strong>k-NN Based:</strong> Measure distance to k-th nearest neighbor (\( D_k(x) \)) (Formula 11) or average distance to k neighbors. Anomalies are expected to be far from their neighbors. Less reliable in high-D due to distance concentration. Using robust distance metrics like Mahalanobis distance (Formula 4: \( D_M(x) = \sqrt{(x - \mu)^T \Sigma^{-1} (x - \mu)} \)) which accounts for correlations can sometimes help, but estimating \( \Sigma \) (Formula 6) is difficult in high-D.</li>
        <li><strong>Local Outlier Factor (LOF):</strong> Compares the local density around a point to the local densities of its neighbors. A point with significantly lower density than its neighbors is considered an outlier. Calculates reachability distance (Formula 12), local reachability density (Formula 13: \( lrd_k(p) \)), and finally LOF (Formula 14: \( LOF_k(p) \)). Suffers from distance concentration and computational cost (\(O(N^2)\) naively) in high dimensions.</li>
        <li><strong>Kernel Density Estimation (KDE):</strong> Estimates the probability density function \( \hat{p}(x) \) (Formula 15: \( \hat{p}(x) = \frac{1}{N h^d} \sum K(\frac{x - x_i}{h}) \)) (Formula 16: Kernel \(K\), Formula 17: Bandwidth \(h\)). Points in low-density regions are outliers. Very difficult to apply effectively in high dimensions due to sparsity.</li>
    </ul>

    <h3>Isolation-Based Methods</h3>
    <p>These methods explicitly try to isolate anomalies, leveraging the idea that anomalies are "few and different" and thus easier to separate from the bulk of the data.</p>
    <ul>
        <li><strong>Isolation Forest:</strong> Builds an ensemble of random trees (Isolation Trees). In each tree, data is recursively partitioned by randomly selecting a feature and a random split value within the feature's range until instances are isolated. Anomalies, being different, tend to be isolated in paths closer to the root (shorter path length \( h(x) \)).
            <ul>
                <li>Anomaly Score: Based on the average path length \( E[h(x)] \) across all trees, normalized by the average path length for a sample size \( n \). Formula 18: \( s(x, n) = 2^{-\frac{E[h(x)]}{c(n)}} \). Score near 1 indicates anomaly, near 0.5 indicates normal. Formulas (19, 20, 21): \( h(x), E[h(x)], c(n) \).</li>
                <li>Advantages: Computationally efficient (\( O(N \log N) \)), requires less memory, works well in high dimensions as it doesn't rely on distance metrics, robust to irrelevant features due to random feature selection.</li>
            </ul>
        </li>
    </ul>
    <div class="svg-diagram">
        <h3>Isolation Forest Concept</h3>
        <svg width="450" height="250" xmlns="http://www.w3.org/2000/svg">
             <defs><style>.node{fill:#eaf2f8; stroke:#aed6f1; rx:5; ry:5;}.leaf{fill:#d5f5e3; stroke:#a9dfbf; rx:5; ry:5;}.anomaly{fill:#f5b7b1; stroke:#e74c3c;}.line{stroke:#555; stroke-width:1;}.label{font-size:10px; text-anchor:middle;}</style><marker id="arrowhead-if" markerWidth="6" markerHeight="4" refX="5" refY="2" orient="auto"><polygon points="0 0, 6 2, 0 4" fill="#555" /></marker></defs>
             <g id="tree1">
                <text x="100" y="20" class="label" font-weight="bold">Isolation Tree 1</text>
                 <rect x="70" y="30" width="60" height="25" class="node"/> <text x="100" y="48" class="label">Split F1</text>
                 <rect x="30" y="80" width="60" height="25" class="node"/> <text x="60" y="98" class="label">Split F3</text>
                 <rect x="110" y="80" width="60" height="25" class="leaf anomaly"/> <text x="140" y="98" class="label anomaly">Anomaly (Isolated)</text>
                 <rect x="0" y="130" width="50" height="25" class="leaf"/> <text x="25" y="148" class="label">Normal</text>
                 <rect x="60" y="130" width="50" height="25" class="leaf"/> <text x="85" y="148" class="label">Normal</text>
                 <line x1="100" y1="55" x2="60" y2="80"/> <line x1="100" y1="55" x2="140" y2="80"/>
                 <line x1="60" y1="105" x2="25" y2="130"/> <line x1="60" y1="105" x2="85" y2="130"/>
                 <text x="80" y="180" class="label">Anomaly: Short Path</text>
            </g>
             <g id="tree2" transform="translate(250,0)">
                 <text x="100" y="20" class="label" font-weight="bold">Isolation Tree 2</text>
                  <rect x="70" y="30" width="60" height="25" class="node"/> <text x="100" y="48" class="label">Split F5</text>
                  <rect x="30" y="80" width="60" height="25" class="leaf anomaly"/> <text x="60" y="98" class="label anomaly">Anomaly (Isolated)</text>
                  <rect x="110" y="80" width="60" height="25" class="node"/> <text x="140" y="98" class="label">Split F2</text>
                  <rect x="80" y="130" width="50" height="25" class="leaf"/> <text x="105" y="148" class="label">Normal</text>
                  <rect x="140" y="130" width="50" height="25" class="leaf"/> <text x="165" y="148" class="label">Normal</text>
                  <line x1="100" y1="55" x2="60" y2="80"/> <line x1="100" y1="55" x2="140" y2="80"/>
                  <line x1="140" y1="105" x2="105" y2="130"/> <line x1="140" y1="105" x2="165" y2="130"/>
                 <text x="80" y="180" class="label">Anomaly: Short Path</text>
                 <text x="140" y="180" class="label">Normal: Longer Path</text>
            </g>
         </svg>
         <p class="caption">Fig 3: Isolation Forest isolates anomalies closer to the root (shorter paths) in random trees.</p>
    </div>

    <h3>Reconstruction-Based Methods (Deep Learning)</h3>
    <p>These methods, often using neural networks, learn a compressed representation of normal data and assume anomalies cannot be accurately reconstructed from this compressed form.</p>
    <ul>
        <li><strong>Autoencoders (AE):</strong> Neural networks trained to reconstruct their input. Consist of an Encoder \( f_{enc} \) mapping input \( x \) to a lower-dimensional latent representation \( z \), and a Decoder \( f_{dec} \) mapping \( z \) back to a reconstruction \( \hat{x} \). Formulas (22, 23): \( z = f_{enc}(x) \), \( \hat{x} = f_{dec}(z) \). Trained on normal data to minimize reconstruction error (e.g., MSE). Formula (24): \( L(x, \hat{x}) = ||x - \hat{x}||^2 \). Anomalies, unseen during training, are expected to yield high reconstruction errors.</li>
        <li><strong>Variational Autoencoders (VAE):</strong> Probabilistic version of AEs, modeling the latent space as a distribution. Anomaly score can be based on reconstruction probability or deviation in the latent space. Loss includes reconstruction term and KL divergence regularizer. Formula (25): \( L_{ELBO} = E_{q(z|x)}[\log p(x|z)] - D_{KL}(q(z|x) || p(z)) \).</li>
        <li><strong>GAN-based Anomaly Detection (e.g., AnoGAN):</strong> Train a GAN on normal data. Anomalies can be detected because: 1) The discriminator might assign low probability to them. 2) The generator cannot produce similar images from the latent space. Anomalies can be detected by mapping a test image to the latent space (finding \( z \) that minimizes reconstruction error \( ||x - G(z)||^2 \)) and using the reconstruction error or discriminator score as the anomaly score.</li>
    </ul>

    <div class="svg-diagram">
        <h3>Autoencoder for Anomaly Detection</h3>
         <svg width="500" height="180" xmlns="http://www.w3.org/2000/svg">
            <defs><marker id="arrowhead-ae" markerWidth="7" markerHeight="5" refX="6" refY="2.5" orient="auto"><polygon points="0 0, 7 2.5, 0 5" fill="#555" /></marker><style>.layer{fill:#d6eaf8; stroke:#aed6f1; rx:5; ry:5;}.latent{fill:#f5f1f9; stroke:#c39bd3; rx:5; ry:5;}.error{fill:#fdedec; stroke:#f5b7b1; rx:5; ry:5;}.label{font-size:10px; text-anchor:middle;}.arrow{stroke:#555; stroke-width:1.5; marker-end:url(#arrowhead-ae);}</style></defs>
            <rect x="10" y="70" width="60" height="40" class="layer"/> <text x="40" y="95" class="label">Input x</text>
            <path d="M 70 90 L 130 90 L 150 110 L 150 70 Z" fill="#aed6f1"/> <text x="110" y="60" class="label">Encoder</text>
            <rect x="100" y="75" width="40" height="30" class="layer"/>
            <rect x="180" y="80" width="40" height="20" class="latent"/> <text x="200" y="95" class="label">Latent z</text>
            <line x1="140" y1="90" x2="180" y2="90" class="arrow"/>
            <path d="M 220 90 L 280 90 L 260 110 L 260 70 Z" fill="#aed6f1" transform="rotate(180 250 90)"/> <text x="250" y="60" class="label">Decoder</text>
             <rect x="280" y="75" width="40" height="30" class="layer"/>
             <line x1="220" y1="90" x2="280" y2="90" class="arrow"/>
            <rect x="350" y="70" width="60" height="40" class="layer"/> <text x="380" y="95" class="label">Output x̂</text>
            <line x1="320" y1="90" x2="350" y2="90" class="arrow"/>
            <rect x="440" y="70" width="50" height="40" class="error"/> <text x="465" y="90" class="label">Error</text> <text x="465" y="100" class="label">||x-x̂||²</text>
             <path d="M 40 70 Q 225 10 465 70" stroke="#aaa" fill="none" stroke-dasharray="3,3"/> <text x="250" y="35" class="label">Compare</text>
            <path d="M 380 110 Q 420 140 465 110" stroke="#aaa" fill="none" stroke-dasharray="3,3"/>
             <text x="400" y="135" class="label">High Error => Anomaly</text>
         </svg>
         <p class="caption">Fig 4: Autoencoder learns to reconstruct normal data; anomalies result in high reconstruction error.</p>
    </div>

    <h3>One-Class Classification Methods</h3>
    <p>These methods learn a boundary around the normal data points. Instances falling outside this boundary are classified as anomalies.</p>
    <ul>
        <li><strong>One-Class SVM (OC-SVM):</strong> An adaptation of Support Vector Machines for unsupervised anomaly detection. It finds a hyperplane (in a high-dimensional feature space defined by a kernel \( \phi \)) that separates the majority of the data points from the origin with maximum margin.
            <ul>
                 <li>Objective (simplified): Minimize \( \frac{1}{2} ||w||^2 + \frac{1}{\nu N} \sum \xi_i - \rho \) subject to constraints. Formula (26, 27, 28): parameters \( w, \rho, \xi_i \), kernel \( \phi \), hyperparameter \( \nu \).</li>
                <li>Decision Function: \( f(x) = \text{sgn}(w \cdot \phi(x) - \rho) \). Points with \( f(x) < 0 \) are anomalies.</li>
            </ul>
        </li>
    </ul>


    <h2><i class="fas fa-clipboard-check icon"></i> Evaluation Metrics for Anomaly Detection</h2>
    <p>Since anomaly detection is often a highly imbalanced problem (few anomalies vs. many normal points), standard accuracy is not suitable. Common metrics include:</p>
    <ul>
        <li><strong>Precision:</strong> Proportion of detected anomalies that are actually anomalies. Formula (29): \( P = \frac{TP}{TP + FP} \).</li>
        <li><strong>Recall (Sensitivity, True Positive Rate):</strong> Proportion of actual anomalies that were correctly detected. Formula (30): \( R = \frac{TP}{TP + FN} \).</li>
        <li><strong>F1-Score:</strong> Harmonic mean of Precision and Recall. Formula (31): \( F1 = 2 \cdot \frac{P \times R}{P + R} \).</li>
        <li><strong>ROC Curve & AUC-ROC:</strong> Plots True Positive Rate vs. False Positive Rate at various thresholds. Area Under the Curve (AUC) summarizes performance.</li>
        <li><strong>Precision-Recall Curve & AUC-PRC:</strong> Plots Precision vs. Recall. Often more informative than ROC for imbalanced datasets. Area Under the PR Curve (AUC-PRC) is a key metric.</li>
    </ul>
    <p>Formulas (32-35): TP, FP, TN, FN (True/False Positives/Negatives).</p>

    <h2><i class="fas fa-flag-checkered icon"></i> Conclusion</h2>
    <p>
        Anomaly detection in high-dimensional data is a critical yet challenging task due to the curse of dimensionality. Standard distance and density-based methods often falter as dimensions increase. Successful techniques typically involve dimensionality reduction (PCA), identifying relevant subspaces, using robust isolation mechanisms (Isolation Forest), or learning representations of normality via reconstruction (Autoencoders, GANs) or boundary description (One-Class SVM). Deep learning methods, particularly autoencoders, have shown great promise in automatically learning relevant features and representations from complex, high-dimensional data. The choice of method depends heavily on the specific characteristics of the data, the nature of expected anomalies, and computational constraints. As datasets continue to grow in size and dimensionality, research into scalable, robust, and interpretable anomaly detection methods will remain a vital area within machine learning.
    </p>
     <p><i>(Formula count check: Includes High-D def, Euclidean Dist, Manhattan Dist, Mahalanobis Dist, Mean, Covariance, PCA Max Var, PCA Projection, PCA Reconstruction, PCA Recon Error, kNN Dist D_k, LOF reach_dist, LOF lrd, LOF N_k(p), LOF formula, KDE formula, Kernel K, Bandwidth h, iForest h(x), iForest E[h(x)], iForest score s, iForest c(n), AE Encoder, AE Decoder, AE Loss, VAE ELBO, OC-SVM Objective (concept), OC-SVM kernel phi, Precision, Recall, F1 Score, TP, FP, TN, FN. Total > 35).</i></p>


    <div class="author-box">
        <h2>About the Author, Architect & Developer</h2>
        <p>
            <strong>Loveleen Narang</strong> is a seasoned leader in the field of Data Science, Machine Learning, and Artificial Intelligence. With extensive experience in architecting and developing cutting-edge AI solutions, Loveleen focuses on applying advanced technologies to solve complex real-world problems, driving efficiency, enhancing compliance, and creating significant value across various sectors, particularly within government and public administration. His work emphasizes building robust, scalable, and secure systems aligned with industry best practices.
        </p>
    </div>

</div>

</body>
</html>